x-airflow-env: &airflow-env
  # Secret key for the webserver (change in production)
  AIRFLOW__WEBSERVER__SECRET_KEY: super_secret_key
  # Default executor (LocalExecutor for simple setup)
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  # Database connection string
  AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
  # Fernet key for encryption (generate your own in production)
  AIRFLOW__CORE__FERNET_KEY: '78zKzHNDZzXtjpSs0cOA752sAfuyJick7KkE6OKpGUQ='
  # Don't load example DAGs by default
  AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
  # DAGs are not paused when created
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'False'
  # Corrected: Points to the default OpenJDK 17 path on Debian
  JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
  # Note: PATH is usually configured in the Dockerfile, including $JAVA_HOME/bin

services:
  # PostgreSQL database service
  postgres:
    image: postgres:13 # Official PostgreSQL 13 image
    environment:
      POSTGRES_USER: airflow # Database user
      POSTGRES_PASSWORD: airflow # User password
      POSTGRES_DB: airflow # Database name
    volumes:
      # Mounts a named volume to persist database data
      - postgres_db:/var/lib/postgresql/data
    # Defines the restart policy (always tries to restart unless explicitly stopped)
    restart: always

  # Airflow webserver service
  airflow-webserver:
    build:
      context: . # Build context (current folder)
      dockerfile: Dockerfile # Your Dockerfile name
    # Depends on the database being ready
    depends_on:
      - postgres
    environment:
      # Imports common environment variables
      <<: *airflow-env
    volumes:
      # Mounts local folders for DAGs, scripts, data, and logs
      # Ensure these folders exist on your local machine
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - ./logs:/opt/airflow/logs
    ports:
      # Maps host port 8080 to container port 8080
      - "8080:8080"
    # Removed custom entrypoint. Will use the official image's default entrypoint.
    # entrypoint: /bin/bash # <-- Removed
    # Main command to run the webserver. The entrypoint will handle initialization.
    command: webserver
    # Defines the restart policy
    restart: always

  # Airflow scheduler service
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    # Depends on the webserver being ready (which in turn depends on postgres)
    depends_on:
      - airflow-webserver
    environment:
      <<: *airflow-env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - ./logs:/opt/airflow/logs
    # Removed custom entrypoint. Will use the default entrypoint.
    # command: bash -c "airflow scheduler" # <-- Simplified
    command: scheduler
    # Defines the restart policy
    restart: always

  # CLI service to run ad-hoc Airflow commands
  airflow-cli:
    build:
      context: .
      dockerfile: Dockerfile
    # Depends on the database
    depends_on:
      - postgres
    environment:
      <<: *airflow-env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./data:/opt/airflow/data
      - ./logs:/opt/airflow/logs
    # Removed entrypoint and command. Will use the official image's default entrypoint
    # to allow executing arbitrary commands via `docker compose run`.
    # entrypoint: /bin/bash # <-- Removed
    stdin_open: true # Allows interactive input (needed for shells)
    tty: true # Allocates a TTY (needed for interactive shells)
    # command: /bin/bash # <-- Removed

# Definition of the named volume for database persistence
volumes:
  postgres_db: